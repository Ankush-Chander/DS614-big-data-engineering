{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DS614 - Big Data Engineering | DAU","text":"<p>Welcome to the course on Big Data Engineering.</p>"},{"location":"#lectures","title":"Lectures","text":"<ul> <li>Storage Engines<ul> <li>Prerequisites</li> <li>Log based storage + Hash indexes</li> <li>SSTables + LSM Trees</li> </ul> </li> </ul>"},{"location":"code/storage_engines/","title":"Storage Engines","text":"<pre><code>\u251c\u2500\u2500 database.csv  # database log file\n\u251c\u2500\u2500 log_based_main_hashmaps.py  # log based storage engine with hashmaps\n\u251c\u2500\u2500 log_based_main.py  # log based storage engine\n\u251c\u2500\u2500 README.md  # this file\n\u2514\u2500\u2500 requirements.txt  # dependencies\n</code></pre>"},{"location":"code/storage_engines/#log-based-storage-engine","title":"Log based storage engine","text":"<p>This is a simple log based storage engine implementation using FastAPI</p> <p>Source : log_based_storage_engine</p>"},{"location":"code/storage_engines/#setup","title":"Setup","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"code/storage_engines/#run","title":"Run","text":"<pre><code>uvicorn log_based_main:app --reload\n</code></pre>"},{"location":"code/storage_engines/#api","title":"API","text":"<pre><code># set key\ncurl -X POST http://localhost:8000/set_db  -H 'accept: application/json'   -H 'Content-Type: application/json'  --data '{\"key\":\"42\",\"val\":\"life\"}'\n\n# get key\ncurl -X GET http://localhost:8000/get_db/42\n# returns `life`\n\n# update key\ncurl -X POST http://localhost:8000/set_db  -H 'accept: application/json'   -H 'Content-Type: application/json'  --data '{\"key\":\"42\",\"val\":\"answer to life\"}'\n\n# get key\ncurl -X GET http://localhost:8000/get_db/42\n# returns `answer to life`\n\n# inspect database\ncat database.csv\n</code></pre>"},{"location":"code/storage_engines/tests/","title":"Concurrent writes","text":"<p>ab -n 1000 -c 1000 -p payload.json -T application/json http://127.0.0.1:8000/set_db</p>"},{"location":"lectures/0storage_prerequisites/","title":"Prerequisites","text":""},{"location":"lectures/0storage_prerequisites/#system-overview","title":"System overview","text":""},{"location":"lectures/0storage_prerequisites/#layered-architecture","title":"Layered architecture","text":"<p>  Image credits: CSAPP</p>"},{"location":"lectures/0storage_prerequisites/#hardware-overview","title":"Hardware overview","text":"<p> Image credits: CSAPP</p> <p>Buses are like nervous system of the computer. Data moves from one place to another via buses. Buses are charecterized by word size as well as bits that can be transfered in a given time.</p> <p>IO devices are what connects a computer to external world. It\"s like humans have 5 senses, computer has IO devices. 4 key IO devices that we will concern ourselves with are:      1. Display: out device through which computer talks back with the user.     2. Keyboard/mouse: input device through which computer listens to the user     3. Storage device: This is the long term storage that computer has. All programs initially lies here.   </p> <p>Main memory is the area where program is loaded when it is to be run and it stays there while it\"s being executed. Think of it like short term memory in humans. Any task in ordered to be done should inside our memory.</p> <p>Processor is where results and addresses are computed in the program. It has 3 main parts: 1. Program counter 2. registers  3. ALU(Arithmetic and Logical unit)   </p>"},{"location":"lectures/0storage_prerequisites/#memory-hierarchy","title":"Memory hierarchy","text":""},{"location":"lectures/0storage_prerequisites/#storage-devices","title":"Storage devices","text":"<ol> <li>Random Access Memory<ol> <li>Static RAM(SRAM) is used for cache memories, both on and off the CPU chip.</li> <li>DRAM(Dynamic RAM) is used for the main memory plus the frame buffer of a graphics system.</li> </ol> </li> <li>HDD(Magnetic Storage) use spinning magnetic platters to store data. A read/write head moves over the platters to read or write data. </li> <li>Solid state disks(SSD) store data on interconnected flash memory chips that retain data even when powered off. </li> </ol>"},{"location":"lectures/0storage_prerequisites/#memory-hierarchy-and-cache","title":"Memory hierarchy and Cache","text":"<p>The storage devices in every computer system are organised as a memory hierarchy. As we move from the top of the hierarchy to the bottom, the devices become slower, larger, and less costly per byte.</p> <p>The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower level. Thus, the register \ufb01le is a cache for the L1 cache. Caches L1 and L2 are caches for L2 and L3, respectively. The L3 cache is a cache for the main memory, which is a cache for the disk. <pre><code>lscpu | grep cache;getconf -a | grep CACHE\n</code></pre></p> <p> Image credits: CSAPP</p>"},{"location":"lectures/0storage_prerequisites/#caching","title":"Caching","text":"<ul> <li>Hardware : Registers, L1, L2, L3  act as cache for main memory.   </li> <li>Operating system: Main memory acts as cache for disc while implementing virtual memory.  </li> <li>Application programs: Browser cache recently accessed web pages for faster loading. </li> </ul>"},{"location":"lectures/0storage_prerequisites/#locality-principles","title":"Locality principles","text":"<p>Cache leads to improved performance because of following principles:</p> <p>Temporal locality:  a memory location that is referenced once is likely to be referenced again multiple times in the near future. Spatial locality:  if a memory location is referenced once, then the program is likely to reference a nearby memory location in the near future.</p>"},{"location":"lectures/0storage_prerequisites/#relative-latencies","title":"Relative latencies","text":"<p> Image credits:  relative-time-latencies-and-computer-programming</p>"},{"location":"lectures/0storage_prerequisites/#disk-access","title":"Disk access","text":"<p>HDD VS SSD</p> <p> Image credits: Backblaze</p> <p>Total Read Time = Seek time  + Rotational latency (HDD only)  + Transfer time (sequential read)</p>"},{"location":"lectures/0storage_prerequisites/#hdd-vs-ssd","title":"HDD vs SSD","text":"Pattern HDD SSD Sequential read Excellent Excellent Random read Terrible Acceptable Seek cost Dominant None Throughput High if sequential High Latency variance Huge Small"},{"location":"lectures/1hash_indexes/","title":"Log Based Storage + Hash Indexes","text":""},{"location":"lectures/1hash_indexes/#quick-recap","title":"Quick recap","text":"<ul> <li>Data Engineer: skills and responsibilities</li> <li>Data Engineering lifecycle</li> </ul>"},{"location":"lectures/1hash_indexes/#agenda","title":"Agenda","text":"<ul> <li>Storage engines<ul> <li>Log based storage</li> <li>Hash indexes</li> </ul> </li> </ul>"},{"location":"lectures/1hash_indexes/#why-study-storage-internals","title":"Why study storage internals?","text":"<ul> <li>To pick right tools for your use case</li> <li>To be able to tune the configurations of storage engine as per your requirements</li> </ul>"},{"location":"lectures/1hash_indexes/#storage-engine-overview","title":"Storage engine overview","text":"Property Transaction processing systems(OLTP) Analytics processing(OLAP) Primarily used for End user/customer, via web application Internal analyst, for decision support What data represents Latest state of data (current point in time) History of events that happened over time Dataset size Gigabytes to terabytes Terabytes to petabytes Main write pattern Random-access, low-latency writes from user input Bulk import (ETL) or event stream Main read pattern Small number of records per query, fetched by key Aggregate over large number of records Implementation types - Log structured database- B tree based databases - Columnar storage"},{"location":"lectures/1hash_indexes/#log-based-storage","title":"Log based storage","text":"<p>Refer code</p> <p>A log is an append-only sequence of records. It doesn\u2019t have to be human-readable; it might be binary and intended only for other programs to read.</p> <pre><code>#!/bin/bash\ndb_set () {\n    echo \"$1,$2\" &gt;&gt; database\n}\n\ndb_get () {\n    grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1\n}\n</code></pre> Pros Cons - Faster writes since appending to a file is a cheap operation - Read complexity is O(n) since each read requires us to read the whole file"},{"location":"lectures/1hash_indexes/#log-based-storage-hash-indexes","title":"Log based storage + hash indexes","text":"<p>Refer code</p> <p>Key idea: Along with appending to the log file maintain a in-memory hashmap that acts as a signpost to the actual keys.</p> <p> Image credits: Designing Data-Intensive Applications</p>"},{"location":"lectures/1hash_indexes/#implementation-details","title":"Implementation details:","text":"<ul> <li>How do we avoid running out of disk space?:   via compaction: Process of timely removing the duplicates from segment file. </li> <li>File format: CSV is not the best format for a log. It\u2019s faster and simpler to use a binary format that first encodes the length of a string in bytes, followed by the raw string (without need for escaping).</li> <li>How to delete record? If you want to delete a key and its associated value, you have to append a special deletion record to the data file (sometimes called a tombstone). When log segments are merged, the tombstone tells the merging process to discard any previous values for the deleted key.</li> <li>Crash recovery:<ul> <li>Read all segment and regenerate hash map on service restart(painful if we have large segment files)</li> <li>Maintain hashmap on disk also and read it into memory on restart.</li> </ul> </li> <li>Concurrency: One write thread, multiple read thread</li> </ul>"},{"location":"lectures/1hash_indexes/#tradeoffs","title":"Tradeoffs","text":"<p>Pros:</p> <ul> <li>Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives.  </li> <li>Concurrency and crash recovery are much simpler if segment files are append-only or immutable.  </li> <li>Merging old segments avoids the problem of data files getting fragmented over time  </li> </ul> <p>Cons:</p> <ul> <li>Hash table must fit into main memory. so if you have a very large number of keys, you\u2019re out of luck.  </li> <li>Range queries are not efficient. For example, you cannot easily scan over all keys between kitty00000 and kitty99999\u2014you\u2019d have to look up each key individu\u2010 ally in the hash maps.  </li> </ul>"},{"location":"lectures/1hash_indexes/#references","title":"References","text":"<ol> <li>Chapter 1, Computer Systems: A Programmer's Perspective(CSAPP)</li> <li>Chapter 3, Designing Data intensive Applications</li> <li>Bitcask - A Log-Structured fast KV store</li> </ol>"},{"location":"lectures/2ssts_lsm_trees/","title":"SSTables + LSM Trees","text":""},{"location":"lectures/2ssts_lsm_trees/#quick-recap","title":"Quick recap","text":"<ul> <li>Log based storage + in-memory Hashmaps<ul> <li>Engines powered by logs(append only records) are write efficient</li> <li>Read efficiency is improved via in memory hashmaps(key=&gt;byte offsets)</li> <li>Binary files(byte streams) are preferred over text files for storing log</li> <li>Logs grow very fast hence broken into segments and occassionally compacted </li> <li>Storage engine has lots of shared resources, hence concurrency management  </li> </ul> </li> <li>Key Questions still unanswered<ul> <li>What if number of keys exceeds RAM rendering in-memory hash maps infeasible</li> <li>How do we scan all keys within a range?</li> </ul> </li> </ul>"},{"location":"lectures/2ssts_lsm_trees/#sstables-key-characters","title":"SSTables: Key characters","text":"<p> Image Credits: Google Gemini</p> <ul> <li>Sorted String tables: All logs storing the key value pairs are sorted by keys</li> <li>Sorted String indexes: Sparse indexes of all sorted segments are stored in-memory</li> <li>Memtable: An in-memory structure that allows keys to be stored in a sorted manner. All new keys are stored in memtable and eventually flushed into disc as an SST segment</li> <li>Write ahead log(WAL): To prevent data loss in case of service crash/shutdown. memtable contents are also written on disc as a backup</li> <li>Bloom Filter: A probabilistic data structure that helps in quickly ruling out whether a key may be found in a segment or sure to be absent in it.</li> </ul>"},{"location":"lectures/2ssts_lsm_trees/#write-path","title":"Write path","text":"<ul> <li>All keys are first written in memtable.</li> <li>When memtable exceeds a certain threshold,  it is flushed into a sorted segment(along with corresponding bloom filter as well as sparse index)</li> <li>Memtable ensures that all keys are unique in a given Segment</li> <li>Segments are merged in background</li> </ul> <p>Image Credits: Designing Data Intensive Applications</p>"},{"location":"lectures/2ssts_lsm_trees/#read-path","title":"Read path","text":"<ul> <li>Check MemTable: Is the key in the active memory buffer? If yes, return immediately (fastest).</li> <li>Check Immutable MemTable: If there is a MemTable currently being flushed to disk, check there.</li> <li>Check SSTables (L0, L1, etc.): If not in memory, the database must check the disk files. It searches the SSTables from newest to oldest.<ul> <li>Optimization: Before reading the actual file, it checks the Bloom Filter. If the filter says \"Key not present,\" it skips that SSTable entirely.</li> <li>If the filter returns positive, it uses the Sparse Index to find the specific block in the file to read.</li> </ul> </li> </ul>"},{"location":"lectures/2ssts_lsm_trees/#mergingcompaction-decisions","title":"Merging/Compaction decisions","text":""},{"location":"lectures/2ssts_lsm_trees/#size-tiered-compaction-stc","title":"Size-Tiered Compaction (STC)","text":"<p>Used by: HBase, Cassandra (default), ScyllaDB</p> <p>The philosophy here is: \"Delay the work.\" It is lazy. It wants to accept writes as fast as possible and only organizes files when it absolutely has to.</p>"},{"location":"lectures/2ssts_lsm_trees/#how-it-works-the-snowball-effect","title":"How it works (The \"Snowball\" Effect)","text":"<p>Imagine you have buckets of different sizes on your disk. 1. Flush: MemTables are flushed to disk as small SSTables (Tier 1). 2. Accumulate: The system waits until it has, say, 4 small SSTables of similar size. 3. Merge: It triggers a compaction to merge these 4 small files into 1 medium SSTable (Tier 2). 4. Repeat: It waits until it has 4 medium SSTables, then merges them into 1 large SSTable (Tier 3).  </p> <p>Pros: -  Write Optimized: Excellent for write-heavy workloads (like IoT logs). The system does minimal re-writing. Cons: -  High Read Amplification: This is the big downside. To find a key, you might have to check many different files because the keys are not neatly separated. You might have <code>kitty1</code> in the small file, the medium file, and the large file. -  Space Spikes: Merging a \"Large Tier\" is painful. If you have four 10GB files, you need to write a new 40GB file. During the merge, you temporarily need 80GB of disk space (40GB input + 40GB output).  </p>"},{"location":"lectures/2ssts_lsm_trees/#leveled-compaction-lc","title":"Leveled Compaction (LC)","text":"<p>Used by: LevelDB, RocksDB, Cassandra (optional) </p> <p>The philosophy here is: \"Keep it clean constantly.\" It is strict. It does extra work during writes to ensure reads are super fast later.  </p>"},{"location":"lectures/2ssts_lsm_trees/#how-it-works-the-bookshelf-method","title":"How it works (The \"Bookshelf\" Method)","text":"<p>The disk is divided into Levels (L0, L1, L2...), each with a target size limit (e.g., L1 is 100MB, L2 is 1GB).  </p> <p>Crucially, in L1 and deeper, the system guarantees that keys do not overlap between files.  </p> <ul> <li>File A: <code>apple</code> ... <code>cat</code> </li> <li>File B: <code>dog</code> ... <code>fish</code> </li> <li>File C: <code>goat</code> ... <code>zebra</code> </li> </ul> <p>You will never find <code>cat</code> in File B.  </p> <p>The \"Incremental\" Merge </p> <p>\"key range is split up into smaller SSTables... allows the compaction to proceed more incrementally\" </p> <p>This is the genius of Leveled Compaction.  </p> <ol> <li>Trigger: L1 fills up. We need to move data to L2.</li> <li>Pick a Victim: We pick one file from L1 (e.g., File A: <code>apple</code>...<code>cat</code>).</li> <li>Find Overlap: We look at L2. We don't need to merge all of L2. We only pick the specific files in L2 that overlap with <code>apple</code>...<code>cat</code>.  </li> <li>Merge: We merge just those few files and write the result back to L2.  </li> </ol> <p>Because we only touch a small slice of the data, we don't need massive temporary disk space (addressing the \"use less disk space\" claim).  </p> <p>Pros - Read Optimized: Best for user-facing apps. Since keys don't overlap in deeper levels, a read query hits exactly one file per level (excluding L0).  -  Space Efficient: You don't need 2x disk space for compactions; you only need enough space for the small slice you are currently merging.   </p> <p>Cons  - High Write Amplification: The database is constantly moving data. A single piece of data might be read and re-written 10-20 times as it trickles down from L1 to L6. This burns CPU and SSD lifetime.   </p>"},{"location":"lectures/2ssts_lsm_trees/#summary-comparison-table","title":"Summary Comparison Table","text":"Size-Tiered (HBase/Cassandra) Leveled (RocksDB/LevelDB) Logic Merge similar-sized files together. Merge files into strictly sorted levels. Key Overlap High. Keys can exist in many files. None (except L0). Keys are partitioned. Read Speed Slower. Must check many files. Fast. Checks very few files. Write Speed Fast. Low write amplification. Slower. High write amplification. Disk Space High Overhead. Needs ~50% free space for big merges. Low Overhead. Incremental merges use little space. Best For Logging, Time-Series, Write-Heavy. General Purpose, Read-Heavy, Low Latency."},{"location":"lectures/2ssts_lsm_trees/#references","title":"References","text":"<ol> <li>SSTable and Log Structured Storage: LevelDB - igvita.com</li> <li>Chapter 3, Designing Data Intensive Applications</li> <li>Bloom Filters </li> </ol>"}]}